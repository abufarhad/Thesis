{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train the model\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "\n",
    "import model.net as net\n",
    "from model.data_loader import DataLoader\n",
    "from evaluate import evaluate\n",
    "\n",
    "import  easydict \n",
    "from easydict import EasyDict \n",
    "\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, data_iterator, metrics, params, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = utils.RunningAverage()\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps) \n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "        train_batch, labels_batch = next(data_iterator)\n",
    "\n",
    "        # compute model output and loss\n",
    "        output_batch = model(train_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate summaries only once in a while\n",
    "        if i % params.save_summary_steps == 0:\n",
    "            # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "            output_batch = output_batch.data.cpu().numpy()\n",
    "            labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "            # compute all metrics on this batch\n",
    "            summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
    "                             for metric in metrics}\n",
    "            summary_batch['loss'] = loss.item() #loss.data[0]\n",
    "            summ.append(summary_batch)\n",
    "\n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.item())#loss.data[0])\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    logging.info(\"- Train metrics: \" + metrics_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, restore_file=None):\n",
    "    \"\"\"Train the model and evaluate every epoch.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        train_data: (dict) training data with keys 'data' and 'labels'\n",
    "        val_data: (dict) validaion data with keys 'data' and 'labels'\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        restore_file: (string) optional- name of file to restore from (without its extension .pth.tar)\n",
    "    \"\"\"\n",
    "    # reload weights from restore_file if specified\n",
    "    if restore_file is not None:\n",
    "        restore_path = os.path.join(args.model_dir, args.restore_file + '.pth')\n",
    "        logging.info(\"Restoring parameters from {}\".format(restore_path))\n",
    "        utils.load_checkpoint(restore_path, model, optimizer)\n",
    "        \n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(params.num_epochs):\n",
    "        # Run one epoch\n",
    "        logging.info(\"Epoch {}/{}\".format(epoch + 1, params.num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (params.train_size + 1) // params.batch_size\n",
    "        train_data_iterator = data_loader.data_iterator(train_data, params, shuffle=True)\n",
    "        train(model, optimizer, loss_fn, train_data_iterator, metrics, params, num_steps)\n",
    "            \n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (params.val_size + 1) // params.batch_size\n",
    "        val_data_iterator = data_loader.data_iterator(val_data, params, shuffle=False)\n",
    "        val_metrics = evaluate(model, loss_fn, val_data_iterator, metrics, params, num_steps)\n",
    "        \n",
    "        val_acc = val_metrics['accuracy']\n",
    "        is_best = val_acc >= best_val_acc\n",
    "\n",
    "        # Save weights\n",
    "        utils.save_checkpoint({'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optim_dict' : optimizer.state_dict()}, \n",
    "                               is_best=is_best,\n",
    "                               checkpoint=model_dir)\n",
    "            \n",
    "        # If best_eval, best_save_path        \n",
    "        if is_best:\n",
    "            logging.info(\"- Found new best accuracy\")\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "            # Save best val metrics in a json file in the model directory\n",
    "            best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
    "            utils.save_dict_to_json(val_metrics, best_json_path)\n",
    "\n",
    "        # Save latest val metrics in a json file in the model directory\n",
    "        last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
    "        utils.save_dict_to_json(val_metrics, last_json_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load File :  <_io.TextIOWrapper name='experiments/base_model\\\\params.json' mode='r' encoding='cp1252'>\n",
      "params :  {'learning_rate': 0.001, 'batch_size': 5, 'num_epochs': 5, 'dropout': 0.5, 'number_of_tags': 10, 'bidirectional': 1, 'n_layers': 2, 'lstm_hidden_dim': 40, 'embedding_dim': 20, 'save_summary_steps': 100}\n",
      "Train params :  <utils.Params object at 0x000001774DD71308>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load File :  <_io.TextIOWrapper name='data\\\\dataset_params.json' mode='r' encoding='cp1252'>\n",
      "params :  {'train_size': 304, 'dev_size': 65, 'test_size': 66, 'vocab_size': 21, 'number_of_classes': 10, 'pad_word': '<pad>'}\n",
      "loading vocab.....\n",
      " l = i  M 0\n",
      " l = i  H 1\n",
      " l = i  I 2\n",
      " l = i  N 3\n",
      " l = i  E 4\n",
      " l = i  T 5\n",
      " l = i  D 6\n",
      " l = i  W 7\n",
      " l = i  L 8\n",
      " l = i  V 9\n",
      " l = i  K 10\n",
      " l = i  P 11\n",
      " l = i  A 12\n",
      " l = i  S 13\n",
      " l = i  F 14\n",
      " l = i  R 15\n",
      " l = i  G 16\n",
      " l = i  Q 17\n",
      " l = i  Y 18\n",
      " l = i  C 19\n",
      " l = i  <pad> 20\n",
      "data_loader  <model.data_loader.DataLoader object at 0x000001774DD40D48>\n",
      "tarin size , val size  :  304 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- done.\n",
      "Starting training for 5 epoch(s)\n",
      "Epoch 1/5\n",
      "100%|██████████████████████████████████████████████████████████████████████| 61/61 [04:09<00:00,  5.17s/it, loss=2.304]\n",
      "- Train metrics: accuracy: 0.200 ; loss: 2.258\n",
      "- Eval metrics : accuracy: 0.092 ; loss: 2.330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best accuracy\n",
      "Epoch 2/5\n",
      "100%|██████████████████████████████████████████████████████████████████████| 61/61 [04:57<00:00,  5.30s/it, loss=2.272]\n",
      "- Train metrics: accuracy: 0.200 ; loss: 2.286\n",
      "- Eval metrics : accuracy: 0.077 ; loss: 2.329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "100%|██████████████████████████████████████████████████████████████████████| 61/61 [04:18<00:00,  5.39s/it, loss=2.263]\n",
      "- Train metrics: accuracy: 0.200 ; loss: 2.253\n",
      "- Eval metrics : accuracy: 0.092 ; loss: 2.321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best accuracy\n",
      "Epoch 4/5\n",
      "100%|██████████████████████████████████████████████████████████████████████| 61/61 [04:21<00:00,  5.23s/it, loss=2.222]\n",
      "- Train metrics: accuracy: 0.200 ; loss: 2.189\n",
      "- Eval metrics : accuracy: 0.092 ; loss: 2.295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best accuracy\n",
      "Epoch 5/5\n",
      "100%|██████████████████████████████████████████████████████████████████████| 61/61 [04:14<00:00,  5.35s/it, loss=2.177]\n",
      "- Train metrics: accuracy: 0.200 ; loss: 2.165\n",
      "- Eval metrics : accuracy: 0.092 ; loss: 2.258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best accuracy\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Load the parameters from json file\n",
    "    #args = parser.parse_args()\n",
    "    \n",
    "    ''' \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', default='data', help=\"Directory containing the dataset\")\n",
    "    parser.add_argument('--model_dir', default='experiments/base_model', help=\"Directory containing params.json\")\n",
    "    parser.add_argument('--restore_file', default=None,\n",
    "                        help=\"Optional, name of the file in --model_dir containing weights to reload before \\\n",
    "                        training\")  # 'best' or 'train'\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "    args = easydict.EasyDict({\n",
    "    \"model_dir\": \"experiments/base_model\",\n",
    "    \"restore_file\": None,\n",
    "    \"data_dir\": \"data\"\n",
    "    })\n",
    "\n",
    "    \n",
    "    json_path = os.path.join(args.model_dir, 'params.json')\n",
    "    assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "    params = utils.Params(json_path)  #Convert JSON data to a Python dictionary\n",
    "    \n",
    "    print(\"Train params : \", params)\n",
    "\n",
    "    # use GPU if available\n",
    "    params.cuda = torch.cuda.is_available()\n",
    "    \n",
    "    # Set the random seed for reproducible experiments\n",
    "    torch.manual_seed(230)\n",
    "    if params.cuda: torch.cuda.manual_seed(230)\n",
    "        \n",
    "    # Set the logger\n",
    "    utils.set_logger(os.path.join(args.model_dir, 'train.log'))\n",
    "    \n",
    "\n",
    "    # Create the input data pipeline\n",
    "    logging.info(\"Loading the datasets...\")\n",
    "    \n",
    "    # load data\n",
    "    data_loader = DataLoader(args.data_dir, params)\n",
    "    print(\"data_loader \", data_loader)\n",
    "    data = data_loader.load_data( ['train', 'val'], args.data_dir)\n",
    "    \n",
    "    #print(\"Data \", data)\n",
    "    train_data = data['train']\n",
    "    val_data = data['val']\n",
    "    \n",
    "    print(\"tarin size , val size  : \", train_data['size'], val_data['size'])\n",
    "\n",
    "    # specify the train and val dataset sizes\n",
    "    params.train_size = train_data['size']\n",
    "    params.val_size = val_data['size']\n",
    "\n",
    "    logging.info(\"- done.\")\n",
    "    \n",
    "    \n",
    "    # Define the model and optimizer\n",
    "    model = net.Net(params).cuda() if params.cuda else net.Net(params)\n",
    "    torch.save(model, 'model_for_visulization.pth')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "    \n",
    "   \n",
    "    \n",
    "    # fetch loss function and metrics\n",
    "    loss_fn = net.loss_fn\n",
    "    metrics = net.metrics\n",
    "\n",
    "    # Train the model\n",
    "    logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "    train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, args.model_dir,\n",
    "                       args.restore_file)\n",
    "                       \n",
    "                      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
