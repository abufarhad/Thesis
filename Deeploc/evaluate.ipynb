{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating the dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocab.....\n",
      " l = i  M 0\n",
      " l = i  H 1\n",
      " l = i  I 2\n",
      " l = i  N 3\n",
      " l = i  E 4\n",
      " l = i  T 5\n",
      " l = i  D 6\n",
      " l = i  W 7\n",
      " l = i  L 8\n",
      " l = i  V 9\n",
      " l = i  K 10\n",
      " l = i  P 11\n",
      " l = i  A 12\n",
      " l = i  S 13\n",
      " l = i  F 14\n",
      " l = i  R 15\n",
      " l = i  G 16\n",
      " l = i  Q 17\n",
      " l = i  Y 18\n",
      " l = i  C 19\n",
      " l = i  <pad> 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- done.\n",
      "Starting evaluation\n",
      "- Eval metrics : accuracy: 0.323 ; loss: 2.074\n"
     ]
    }
   ],
   "source": [
    "# %load evaluate.py\n",
    "\"\"\"Evaluates the model\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import  easydict \n",
    "from easydict import EasyDict \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "import model.net as net\n",
    "from model.data_loader import DataLoader\n",
    "\n",
    "import  easydict \n",
    "from easydict import EasyDict \n",
    "\n",
    "''' \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', default='data', help=\"Directory containing the dataset\")\n",
    "parser.add_argument('--model_dir', default='experiments/base_model', help=\"Directory containing params.json\")\n",
    "parser.add_argument('--restore_file', default='best', help=\"name of the file in --model_dir \\\n",
    "                     containing weights to load\")\n",
    "'''\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(model, loss_fn, data_iterator, metrics, params, num_steps):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # summary for current eval loop\n",
    "    summ = []\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        # fetch the next evaluation batch\n",
    "        data_batch, labels_batch = next(data_iterator)\n",
    "        \n",
    "        # compute model output\n",
    "        output_batch = model(data_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "        output_batch = output_batch.data.cpu().numpy()\n",
    "        labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "        # compute all metrics on this batch\n",
    "        summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                         for metric in metrics}\n",
    "        summary_batch['loss'] = loss.item()#loss.data[0]\n",
    "        summ.append(summary_batch)\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    logging.info(\"- Eval metrics : \" + metrics_string)\n",
    "    return metrics_mean\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "        Evaluate the model on the test set.\n",
    "    \"\"\"\n",
    "    # Load the parameters\n",
    "    #args = parser.parse_args()\n",
    "    \n",
    "    args = easydict.EasyDict({\n",
    "    \"restore_file\": 'best',\n",
    "    \"model_dir\": 'experiments/base_model',\n",
    "    \"data_dir\": \"data\"\n",
    "    })\n",
    "    \n",
    "    json_path = os.path.join(args.model_dir, 'params.json')\n",
    "    assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "    params = utils.Params(json_path)\n",
    "    \n",
    "    #print(json_path, params)\n",
    "\n",
    "    # use GPU if available\n",
    "    params.cuda = torch.cuda.is_available()     # use GPU is available\n",
    "\n",
    "    # Set the random seed for reproducible experiments\n",
    "    torch.manual_seed(230)\n",
    "    if params.cuda: torch.cuda.manual_seed(230)\n",
    "        \n",
    "    # Get the logger\n",
    "    utils.set_logger(os.path.join(args.model_dir, 'evaluate.log'))\n",
    "\n",
    "    # Create the input data pipeline\n",
    "    logging.info(\"Creating the dataset...\")\n",
    "\n",
    "    # load data\n",
    "    data_loader = DataLoader(args.data_dir, params)\n",
    "    data = data_loader.load_data(['test'], args.data_dir)\n",
    "    test_data = data['test']\n",
    "\n",
    "    # specify the test set size\n",
    "    params.test_size = test_data['size']\n",
    "    test_data_iterator = data_loader.data_iterator(test_data, params)\n",
    "\n",
    "    logging.info(\"- done.\")\n",
    "\n",
    "    # Define the model\n",
    "    model = net.Net(params).cuda() if params.cuda else net.Net(params)\n",
    "    \n",
    "    loss_fn = net.loss_fn\n",
    "    metrics = net.metrics\n",
    "    \n",
    "    logging.info(\"Starting evaluation\")\n",
    "\n",
    "    # Reload weights from the saved file\n",
    "    utils.load_checkpoint(os.path.join(args.model_dir, args.restore_file + '.pth'), model)\n",
    "\n",
    "    # Evaluate\n",
    "    num_steps = (params.test_size + 1) // params.batch_size\n",
    "    test_metrics = evaluate(model, loss_fn, test_data_iterator, metrics, params, num_steps)\n",
    "    save_path = os.path.join(args.model_dir, \"metrics_test_{}.json\".format(args.restore_file))\n",
    "    utils.save_dict_to_json(test_metrics, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
