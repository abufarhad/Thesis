{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Build vocabularies of amino acids and classe from datasets\"\"\"\n",
    "\n",
    "import argparse\n",
    "import  easydict \n",
    "from easydict import EasyDict \n",
    "from collections import Counter\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--min_count_word', default=1, help=\"Minimum count for amino acids in the dataset\", type=int)\n",
    "parser.add_argument('--min_count_tag', default=1, help=\"Minimum count for labels in the dataset\", type=int)\n",
    "parser.add_argument('--data_dir', default='data', help=\"Directory containing the dataset\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Hyper parameters for the vocab\n",
    "PAD_WORD = '<pad>'\n",
    "PAD_TAG = 'O'\n",
    "UNK_WORD = 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab_to_txt_file(vocab, txt_path):\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        for token in vocab:\n",
    "            #print (\"save_vocab_to_text \", token, vocab )\n",
    "            f.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_json(d, json_path):\n",
    "    \"\"\"Saves dict to json file\n",
    "\n",
    "    Args:\n",
    "        d: (dict)\n",
    "        json_path: (string) path to json file\n",
    "    \"\"\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        #This is a dictionary. A dictionary comprises of two things : Keys and Values.\n",
    "        #‘k:v’ is basically the action to be performed in the for loop.\n",
    "        \n",
    "        d = {k: v for k, v in d.items()}\n",
    "        # json.dump(data, write_file, intendation)\n",
    "        print(\"data \" , d)\n",
    "        print(\"writefile\" ,f)\n",
    "        json.dump(d, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_vocab(txt_path, vocab):\n",
    "   \n",
    "    with open(txt_path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #print (\"update vocab \", i, line )\n",
    "            vocab.update(line.strip())\n",
    "            #print(\"vocab \", vocab )\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_label(txt_path, vocab):\n",
    "    with open(txt_path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #print (\"update label \", i, line )\n",
    "            vocab.update(line.strip().split())\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building word vocabulary...\n",
      "chars Counter()\n",
      "-> done. Train , Validation, Test size  :  304 65 66\n",
      "Building tag vocabulary...\n",
      "->done Train , Validation, Test label  304 65 66\n",
      "Most Frequent token :  ['M', 'H', 'I', 'N', 'E', 'T', 'D', 'W', 'L', 'V', 'K', 'P', 'A', 'S', 'F', 'R', 'G', 'Q', 'Y', 'C'] ['Endoplasmic.reticulum', 'Golgi.apparatus', 'Cell.membrane', 'Extracellular', 'Cytoplasm', 'Nucleus', 'Plastid', 'Peroxisome', 'Lysosome/Vacuole', 'Mitochondrion']\n",
      "After pad  ['M', 'H', 'I', 'N', 'E', 'T', 'D', 'W', 'L', 'V', 'K', 'P', 'A', 'S', 'F', 'R', 'G', 'Q', 'Y', 'C', '<pad>']\n",
      "Saving vocabularies to file...\n",
      "- done.\n",
      "data  {'train_size': 304, 'dev_size': 65, 'test_size': 66, 'vocab_size': 21, 'number_of_classes': 10, 'pad_word': '<pad>'}\n",
      "writefile <_io.TextIOWrapper name='data\\\\dataset_params.json' mode='w' encoding='cp1252'>\n",
      "Characteristics of the dataset:\n",
      "- train_size: 304\n",
      "- dev_size: 65\n",
      "- test_size: 66\n",
      "- vocab_size: 21\n",
      "- number_of_classes: 10\n",
      "- pad_word: <pad>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #args = parser.parse_args()\n",
    "   \n",
    "    args = easydict.EasyDict({\n",
    "    \"min_count_word\": 1,\n",
    "    \"min_count_tag\": 1,\n",
    "    \"data_dir\": \"data\"\n",
    "})\n",
    "    \n",
    "\n",
    "    # Build word vocab with train and test datasets\n",
    "    print(\"Building word vocabulary...\")\n",
    "    chars = Counter()\n",
    "    print(\"chars\", chars)\n",
    "    size_train_sentences = update_vocab(os.path.join(args.data_dir, 'train/sentences.txt'), chars)\n",
    "    \n",
    "    size_dev_sentences = update_vocab(os.path.join(args.data_dir, 'val/sentences.txt'), chars)\n",
    "    \n",
    "    size_test_sentences = update_vocab(os.path.join(args.data_dir, 'test/sentences.txt'), chars)\n",
    "    \n",
    "    print(\"-> done. Train , Validation, Test size  : \", size_train_sentences, size_dev_sentences,size_test_sentences )\n",
    "\n",
    "    # Build tag vocab with train and test datasets\n",
    "    print(\"Building tag vocabulary...\")\n",
    "    classes = Counter()\n",
    "    size_train_tags = update_label(os.path.join(args.data_dir, 'train/labels.txt'), classes)\n",
    "    size_dev_tags = update_label(os.path.join(args.data_dir, 'val/labels.txt'), classes)\n",
    "    size_test_tags = update_label(os.path.join(args.data_dir, 'test/labels.txt'), classes)\n",
    "    print(\"->done Train , Validation, Test label \" ,size_train_tags,  size_dev_tags,size_test_tags )\n",
    "\n",
    "    # Assert same number of examples in datasets\n",
    "    assert size_train_sentences == size_train_tags\n",
    "    assert size_dev_sentences == size_dev_tags\n",
    "    assert size_test_sentences == size_test_tags\n",
    "\n",
    "    # Only keep most frequent tokens\n",
    "    chars = [tok for tok, count in chars.items() if count >= args.min_count_word]\n",
    "    classes = [tok for tok, count in classes.items() if count >= args.min_count_tag]\n",
    "    \n",
    "    print(\"Most Frequent token : \", chars, classes)\n",
    "\n",
    "    # Add pad tokens\n",
    "    if PAD_WORD not in chars: chars.append(PAD_WORD)\n",
    "        \n",
    "    print(\"After pad \", chars)\n",
    "\n",
    "    # Save vocabularies to file\n",
    "    print(\"Saving vocabularies to file...\")\n",
    "    save_vocab_to_txt_file(chars, os.path.join(args.data_dir, 'chars.txt'))\n",
    "    save_vocab_to_txt_file(classes, os.path.join(args.data_dir, 'classes.txt'))\n",
    "    print(\"- done.\")\n",
    "\n",
    "    # Save datasets properties in json file\n",
    "    sizes = {\n",
    "        'train_size': size_train_sentences,\n",
    "        'dev_size': size_dev_sentences,\n",
    "        'test_size': size_test_sentences,\n",
    "        'vocab_size': len(chars),\n",
    "        'number_of_classes': len(classes),\n",
    "        'pad_word': PAD_WORD,\n",
    "    }\n",
    "    \n",
    "    #we use JSON to store and exchange data\n",
    "    # It’s nothing more than a standardized format the community uses to pass data around\n",
    "    save_dict_to_json(sizes, os.path.join(args.data_dir, 'dataset_params.json'))\n",
    "\n",
    "    # Logging sizes\n",
    "    to_print = \"\\n\".join(\"- {}: {}\".format(k, v) for k, v in sizes.items())\n",
    "    \n",
    "    print(\"Characteristics of the dataset:\\n{}\".format(to_print))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
